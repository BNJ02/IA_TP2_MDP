{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligence Artificielle : Travaux Pratiques sur les Processus de Décision Markoviens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intelligence Artificielle : Travaux Pratiques sur les Processus de Décision Markoviens**\n",
    "\n",
    "L'objectif est d'implémenter et d'évaluer les algorithmes d'itération de valeur et d'itération de politique pour les **Processus de Décision Markoviens** (MDP).\n",
    "\n",
    "Nous étudions un petit exemple tiré du livre *Artificial Intelligence: A Modern Approach* de manière aussi générique que possible.\n",
    "\n",
    "Nous modélisons l'environnement comme une zone rectangulaire carrelée de longueur **L** et de hauteur **H**. Vous pouvez réutiliser et adapter les structures de données et les fonctions du TP sur la recherche de chemin (*Path-finding lab*). Nous nous concentrons en particulier sur le petit environnement de **6×5** suivant :\n",
    "\n",
    "![Environnement 6x5](env.png)\n",
    "\n",
    "Le nombre dans chaque case représente la récompense immédiate obtenue en s'y déplaçant.  \n",
    "Les cases noires sont des murs infranchissables. Notez que, comme dans le TP sur la recherche de chemin, nous supposons que l'environnement est entouré de murs.  \n",
    "Les cases avec des récompenses **+1** et **-1** sont des **nœuds terminaux** : lorsque le robot les atteint, il ne peut plus se déplacer. Par conséquent, l'utilité de ces cases (à partir de l'itération 1) est simplement la récompense immédiate.\n",
    "\n",
    "Nous supposons qu'à chaque fois que le robot essaie de se déplacer dans une direction, il y a **10 % de chances** qu'il aille à gauche (par rapport à la direction choisie) au lieu d'aller tout droit, et **10 % de chances** qu'il aille à droite (toujours par rapport à la direction choisie). Si cela le fait se heurter à un mur, il reste sur place (ne bouge pas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithme\n",
    "\n",
    "![Algo Value Iter](value_iter_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Questions :**\n",
    "\n",
    "**Q1.** Implémentez l'algorithme d'**itération de valeur** pour le MDP correspondant. Affichez la politique calculée à chaque itération en utilisant, par exemple, les caractères `'v'`, `'<'`, `'>'`, `'^'`. Affichez également le **nombre d'itérations nécessaires pour converger**. Utilisez les valeurs **γ = 0.99** et **ϵ = 0.01**.  \n",
    "\n",
    "À la fin du calcul, affichez également les utilités calculées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les actions principales et adjacentes\n",
    "actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "arrows_actions = ['↑', '↓', '←', '→']\n",
    "\n",
    "# Définir les probabilités des actions principales et secondaires\n",
    "action_probs = {\n",
    "    'UP': (0.8, 0.1, 0.1),       # (Proba UP, Proba LEFT, Proba RIGHT)\n",
    "    'DOWN': (0.8, 0.1, 0.1),     # (Proba DOWN, Proba LEFT, Proba RIGHT)\n",
    "    'LEFT': (0.8, 0.1, 0.1),     # (Proba LEFT, Proba UP, Proba DOWN)\n",
    "    'RIGHT': (0.8, 0.1, 0.1)     # (Proba RIGHT, Proba UP, Proba DOWN)\n",
    "}\n",
    "\n",
    "# Association des actions adjacentes\n",
    "adjacent_actions = {\n",
    "    'UP': ['LEFT', 'RIGHT'],     # Mouvement principal UP et erreurs LEFT/RIGHT\n",
    "    'DOWN': ['LEFT', 'RIGHT'],   # Mouvement principal DOWN et erreurs LEFT/RIGHT\n",
    "    'LEFT': ['UP', 'DOWN'],      # Mouvement principal LEFT et erreurs UP/DOWN\n",
    "    'RIGHT': ['UP', 'DOWN']      # Mouvement principal RIGHT et erreurs UP/DOWN\n",
    "}\n",
    "\n",
    "# Fonction pour appliquer une action, avec gestion des murs et des bords\n",
    "def get_next_state(state, action, rewards, H, L):\n",
    "    x, y = state\n",
    "\n",
    "    if action == 'UP':\n",
    "        next_state = (max(0, x - 1), y)\n",
    "    elif action == 'DOWN':\n",
    "        next_state = (min(H - 1, x + 1), y)\n",
    "    elif action == 'LEFT':\n",
    "        next_state = (x, max(0, y - 1))\n",
    "    elif action == 'RIGHT':\n",
    "        next_state = (x, min(L - 1, y + 1))\n",
    "    else:\n",
    "        next_state = state  # Sécurité\n",
    "\n",
    "    # Si on frappe un mur, rester sur place\n",
    "    if np.isnan(rewards[next_state[0], next_state[1]]):\n",
    "        return state\n",
    "\n",
    "    return next_state\n",
    "\n",
    "# Fonction pour vérifier si un état est terminal\n",
    "def is_terminal(state, rewards):\n",
    "    x, y = state\n",
    "    return rewards[x][y] in [1, -1]\n",
    "\n",
    "# Implémentation de l'algorithme Value Iteration\n",
    "def value_iteration(gamma, epsilon, rewards, H, L):\n",
    "    utilities = np.zeros((H, L))  # Initialisation des valeurs d'utilité\n",
    "\n",
    "    # Terminaux\n",
    "    utilities[0][3] = 1    # Récompense +1\n",
    "    utilities[1][3] = -1   # Récompense -1\n",
    "\n",
    "    policy = np.full((H, L), '')  # Initialisation de la politique\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        delta = 0\n",
    "        new_utilities = np.copy(utilities)\n",
    "        \n",
    "        for x in range(H):\n",
    "            for y in range(L):\n",
    "                if is_terminal((x, y), rewards) or np.isnan(rewards[x][y]):  # Ignorer les états terminaux et les murs\n",
    "                    continue\n",
    "\n",
    "                # Calcul de l'utilité pour chaque action\n",
    "                action_utilities = []\n",
    "                for action in actions:\n",
    "                    # Récupérer les probabilités et mouvements associés\n",
    "                    prob_main, prob_adj1, prob_adj2 = action_probs[action]\n",
    "                    adj1, adj2 = adjacent_actions[action]\n",
    "\n",
    "                    # Calculer les états suivants pour cette action\n",
    "                    next_main = get_next_state((x, y), action, rewards, H, L)\n",
    "                    next_adj1 = get_next_state((x, y), adj1, rewards, H, L)\n",
    "                    next_adj2 = get_next_state((x, y), adj2, rewards, H, L)\n",
    "\n",
    "                    # Utilité escomptée pour cette action\n",
    "                    expected_utility = (\n",
    "                        prob_main * utilities[next_main[0], next_main[1]] +\n",
    "                        prob_adj1 * utilities[next_adj1[0], next_adj1[1]] +\n",
    "                        prob_adj2 * utilities[next_adj2[0], next_adj2[1]]\n",
    "                    )\n",
    "                    action_utilities.append(expected_utility)\n",
    "\n",
    "                # Mise à jour de l'utilité avec la meilleure action\n",
    "                best_action_utility = max(action_utilities)\n",
    "                new_utilities[x][y] = rewards[x][y] + gamma * best_action_utility\n",
    "                policy[x][y] = arrows_actions[np.argmax(action_utilities)]\n",
    "                delta = max(delta, abs(new_utilities[x][y] - utilities[x][y]))\n",
    "        \n",
    "        utilities = new_utilities\n",
    "        # Vérifier la condition de convergence\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            break\n",
    "    \n",
    "    # Mettre à jour la politique pour les états terminaux et le mur\n",
    "    policy[0][3] = '*'\n",
    "    policy[1][3] = '*'\n",
    "    policy[np.isnan(rewards)] = '#'\n",
    "\n",
    "    return utilities, policy, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04   nan -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "# Définition de l'environnement\n",
    "L, H = 4, 3\n",
    "\n",
    "reward_default = -0.04\n",
    "rewards = np.full((H, L), reward_default)\n",
    "rewards[0][3] = 1    # Récompense +1\n",
    "rewards[1][3] = -1   # Récompense -1\n",
    "\n",
    "# Créer le mur de l'environnement au centre\n",
    "rewards[1, 1] = np.nan\n",
    "\n",
    "# Afficher les récompenses à travers l'environnement\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs d'utilité calculées :\n",
      "[[ 0.7761849   0.8439351   0.9050959   1.        ]\n",
      " [ 0.71662996  0.          0.64132736 -1.        ]\n",
      " [ 0.65064215  0.59261419  0.56005111  0.33800045]]\n",
      "\n",
      "Politique optimale :\n",
      "→ → → *\n",
      "↑ # ↑ *\n",
      "↑ ← ↑ ←\n",
      "\n",
      "Nombre d'itérations nécessaires : 20\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamètres de VIA (Value Iteration Algorithm)\n",
    "gamma = 0.99\n",
    "epsilon = 0.01\n",
    "\n",
    "# Exécuter Value Iteration\n",
    "utilities, policy, iterations = value_iteration(gamma, epsilon, rewards, H, L)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Valeurs d'utilité calculées :\")\n",
    "print(utilities)\n",
    "print(\"\\nPolitique optimale :\")\n",
    "for row in policy:\n",
    "    print(' '.join(row))\n",
    "print(f\"\\nNombre d'itérations nécessaires : {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Q2.** Expérimentez avec d'autres récompenses. Essayez en particulier de faire varier la récompense des cases \"normales\".  \n",
    "- Que se passe-t-il lorsqu'elle est **positive** ?  \n",
    "- Et lorsqu'elle est beaucoup plus négative que **-0.04** (par exemple **-2**) ?  \n",
    "- Pouvez-vous trouver des situations intermédiaires intéressantes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1  0.1  0.1  1. ]\n",
      " [ 0.1  nan  0.1 -1. ]\n",
      " [ 0.1  0.1  0.1  0.1]]\n"
     ]
    }
   ],
   "source": [
    "# Définition de l'environnement\n",
    "L, H = 4, 3\n",
    "\n",
    "reward_default = 0.1\n",
    "rewards = np.full((H, L), reward_default)\n",
    "rewards[0][3] = 1    # Récompense +1\n",
    "rewards[1][3] = -1   # Récompense -1\n",
    "\n",
    "# Créer le mur de l'environnement au centre\n",
    "rewards[1, 1] = np.nan\n",
    "\n",
    "# Afficher les récompenses à travers l'environnement\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs d'utilité calculées :\n",
      "[[ 9.99006829  9.99006829  9.99006829  1.        ]\n",
      " [ 9.99006829  0.          9.99006829 -1.        ]\n",
      " [ 9.99006829  9.99006829  9.99006829  9.99006829]]\n",
      "\n",
      "Politique optimale :\n",
      "↑ ↑ ← *\n",
      "↑ # ← *\n",
      "↑ ↑ ↑ ↓\n",
      "\n",
      "Nombre d'itérations nécessaires : 680\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamètres de VIA (Value Iteration Algorithm)\n",
    "gamma = 0.99\n",
    "epsilon = 0.01\n",
    "\n",
    "# Exécuter Value Iteration\n",
    "utilities, policy, iterations = value_iteration(gamma, epsilon, rewards, H, L)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Valeurs d'utilité calculées :\")\n",
    "print(utilities)\n",
    "print(\"\\nPolitique optimale :\")\n",
    "for row in policy:\n",
    "    print(' '.join(row))\n",
    "print(f\"\\nNombre d'itérations nécessaires : {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cas avec rewards > 0 : cherche à ne pas terminer pour maximiser son gain (ARGENT)\n",
    "- cas avec rewards << 0 : cherche à se suicider, donc terminer au plus vite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration Algorithme\n",
    "\n",
    "![Algo Policy Iteration](policy_iter_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Q3.** Implémentez l'algorithme d'**itération de politique** pour le MDP correspondant. Utilisez une version simplifiée de la fonction écrite pour la question Q1 afin de calculer les utilités des politiques jusqu'à **ϵ**.  \n",
    "\n",
    "Affichez la politique calculée à chaque itération et le **nombre d'itérations nécessaires pour converger**. Comparez avec l'itération de valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation de la politique (Policy Evaluation)\n",
    "def policy_evaluation(policy, gamma, epsilon, rewards, H, L):\n",
    "    utilities = np.zeros((H, L))  # Initialisation des valeurs d'utilité\n",
    "\n",
    "    utilities[0][3] = 1    # Récompense +1\n",
    "    utilities[1][3] = -1   # Récompense -1\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_utilities = np.copy(utilities)\n",
    "        for x in range(H):\n",
    "            for y in range(L):\n",
    "                if is_terminal((x, y), rewards) or np.isnan(rewards[x][y]):  # Ignorer les états terminaux et les murs\n",
    "                    continue\n",
    "                \n",
    "                action = policy[x][y]  # Action choisie par la politique actuelle\n",
    "                if action == '':\n",
    "                    continue  # Si aucune action n'est définie, passez\n",
    "\n",
    "                # Récupérer les probabilités et mouvements associés\n",
    "                prob_main, prob_adj1, prob_adj2 = action_probs[action]\n",
    "                adj1, adj2 = adjacent_actions[action]\n",
    "\n",
    "                # Calculer les états suivants pour cette action\n",
    "                next_main = get_next_state((x, y), action, rewards, H, L)\n",
    "                next_adj1 = get_next_state((x, y), adj1, rewards, H, L)\n",
    "                next_adj2 = get_next_state((x, y), adj2, rewards, H, L)\n",
    "\n",
    "                # Mettre à jour l'utilité de l'état actuel pour cette politique\n",
    "                expected_utility = (\n",
    "                    prob_main * utilities[next_main[0], next_main[1]] +\n",
    "                    prob_adj1 * utilities[next_adj1[0], next_adj1[1]] +\n",
    "                    prob_adj2 * utilities[next_adj2[0], next_adj2[1]]\n",
    "                )\n",
    "                new_utilities[x][y] = rewards[x][y] + gamma * expected_utility\n",
    "                delta = max(delta, abs(new_utilities[x][y] - utilities[x][y]))\n",
    "\n",
    "        utilities = new_utilities\n",
    "        if delta < epsilon:  # Convergence des utilités\n",
    "            break\n",
    "\n",
    "    return utilities\n",
    "\n",
    "# Implémentation de l'algorithme d'itération de politique (Policy Iteration)\n",
    "def policy_iteration(gamma, epsilon, rewards, H, L):\n",
    "    policy = np.full((H, L), 'UP', dtype=object)  # Initialiser une politique arbitraire\n",
    "    policy[0][3] = '*'  # États terminaux\n",
    "    policy[1][3] = '*'\n",
    "    policy[np.isnan(rewards)] = '#'  # Murs\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "\n",
    "        # Étape 1 : Évaluation de la politique actuelle\n",
    "        utilities = policy_evaluation(policy, gamma, epsilon, rewards, H, L)\n",
    "\n",
    "        # Étape 2 : Amélioration de la politique\n",
    "        policy_stable = True\n",
    "        new_policy = np.copy(policy)\n",
    "\n",
    "        for x in range(H):\n",
    "            for y in range(L):\n",
    "                if is_terminal((x, y), rewards) or np.isnan(rewards[x][y]):  # Ignorer les états terminaux et les murs\n",
    "                    continue\n",
    "\n",
    "                # Calcul de l'utilité pour chaque action\n",
    "                action_utilities = []\n",
    "                for action in actions:\n",
    "                    # Récupérer les probabilités et mouvements associés\n",
    "                    prob_main, prob_adj1, prob_adj2 = action_probs[action]\n",
    "                    adj1, adj2 = adjacent_actions[action]\n",
    "\n",
    "                    # Calculer les états suivants pour cette action\n",
    "                    next_main = get_next_state((x, y), action, rewards, H, L)\n",
    "                    next_adj1 = get_next_state((x, y), adj1, rewards, H, L)\n",
    "                    next_adj2 = get_next_state((x, y), adj2, rewards, H, L)\n",
    "\n",
    "                    # Utilité escomptée pour cette action\n",
    "                    expected_utility = (\n",
    "                        prob_main * utilities[next_main[0], next_main[1]] +\n",
    "                        prob_adj1 * utilities[next_adj1[0], next_adj1[1]] +\n",
    "                        prob_adj2 * utilities[next_adj2[0], next_adj2[1]]\n",
    "                    )\n",
    "                    action_utilities.append(expected_utility)\n",
    "\n",
    "                # Déterminer la meilleure action et comparer à la politique courante\n",
    "                best_action_index = np.argmax(action_utilities)\n",
    "                best_action = actions[best_action_index]\n",
    "\n",
    "                # Mettre à jour la politique (changement de direction si nécessaire)\n",
    "                if best_action != policy[x][y]:\n",
    "                    policy_stable = False\n",
    "                    new_policy[x][y] = best_action\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "        # print(f\"Politique après iteration {iterations}:\")\n",
    "        # for row in policy:\n",
    "        #     print(\" \".join(row))\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        if policy_stable:  # Si la politique ne change plus, arrêt\n",
    "            break\n",
    "\n",
    "    return policy, utilities, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04   nan -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "# Définition de l'environnement\n",
    "L, H = 4, 3\n",
    "\n",
    "reward_default = -0.04\n",
    "rewards = np.full((H, L), reward_default)\n",
    "rewards[0][3] = 1    # Récompense +1\n",
    "rewards[1][3] = -1   # Récompense -1\n",
    "\n",
    "# Créer le mur de l'environnement au centre\n",
    "rewards[1, 1] = np.nan\n",
    "\n",
    "# Afficher les récompenses à travers l'environnement\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politique optimale obtenue (itération de politique) :\n",
      "→ → → *\n",
      "↑ # ↑ *\n",
      "↑ ← ↑ ←\n",
      "\n",
      "Valeurs d'utilité correspondantes :\n",
      "[[ 0.77608967  0.84393198  0.90509519  1.        ]\n",
      " [ 0.71632498  0.          0.64132534 -1.        ]\n",
      " [ 0.6486454   0.58727827  0.55845133  0.33504264]]\n",
      "\n",
      "Nombre d'itérations nécessaires : 4\n"
     ]
    }
   ],
   "source": [
    "policy, utilities, iterations = policy_iteration(gamma, epsilon, rewards, H, L)\n",
    "\n",
    "print(\"Politique optimale obtenue (itération de politique) :\")\n",
    "\n",
    "row_to_arrow = {\"UP\": \"↑\", \"DOWN\": \"↓\", \"LEFT\": \"←\", \"RIGHT\": \"→\", \"*\": \"*\", \"#\": \"#\"}\n",
    "for row in policy:\n",
    "    print(\" \".join([row_to_arrow[action] for action in row]))\n",
    "\n",
    "print(\"\\nValeurs d'utilité correspondantes :\")\n",
    "print(utilities)\n",
    "\n",
    "print(f\"\\nNombre d'itérations nécessaires : {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je n'ai pas pris une stratégie aléatoirement au début (j'ai mis tout à UP) en revanche on voit que le policy iteration à bien moins d'itérations que le Value Iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
